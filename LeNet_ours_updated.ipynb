{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ujXXFIO5jB",
        "outputId": "947a184c-19b7-4400-c869-d192ca9466ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 29s 22ms/step - loss: 1.6533 - accuracy: 0.3914\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.3631 - accuracy: 0.5102\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.2355 - accuracy: 0.5613\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 1.1492 - accuracy: 0.5911\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 1.0882 - accuracy: 0.6151\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 1.0365 - accuracy: 0.6325\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.9855 - accuracy: 0.6509\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.9393 - accuracy: 0.6673\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.9048 - accuracy: 0.6799\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.8665 - accuracy: 0.6932\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.8334 - accuracy: 0.7039\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.7992 - accuracy: 0.7164\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.7689 - accuracy: 0.7287\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.7420 - accuracy: 0.7361\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.7176 - accuracy: 0.7458\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6921 - accuracy: 0.7524\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.6579 - accuracy: 0.7641\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6351 - accuracy: 0.7723\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.6160 - accuracy: 0.7800\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.5949 - accuracy: 0.7847\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.5731 - accuracy: 0.7961\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.5489 - accuracy: 0.8019\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.5328 - accuracy: 0.8074\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.5174 - accuracy: 0.8142\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.5012 - accuracy: 0.8212\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.4809 - accuracy: 0.8259\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.4592 - accuracy: 0.8363\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.4519 - accuracy: 0.8371\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.4321 - accuracy: 0.8438\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.4199 - accuracy: 0.8473\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.4073 - accuracy: 0.8518\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3964 - accuracy: 0.8572\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3847 - accuracy: 0.8612\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.3725 - accuracy: 0.8644\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3564 - accuracy: 0.8705\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.3533 - accuracy: 0.8719\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3436 - accuracy: 0.8752\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3365 - accuracy: 0.8783\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.3281 - accuracy: 0.8817\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.3216 - accuracy: 0.8834\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3070 - accuracy: 0.8892\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.2909 - accuracy: 0.8937\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.3020 - accuracy: 0.8908\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2817 - accuracy: 0.8988\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.2813 - accuracy: 0.8984\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2823 - accuracy: 0.8964\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.2737 - accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2702 - accuracy: 0.9039\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.2557 - accuracy: 0.9089\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 0.2600 - accuracy: 0.9058\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 0.2352 - accuracy: 0.9150\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.2534 - accuracy: 0.9077\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.2345 - accuracy: 0.9150\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2510 - accuracy: 0.9097\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.2415 - accuracy: 0.9130\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 0.2225 - accuracy: 0.9175\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2233 - accuracy: 0.9198\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.2233 - accuracy: 0.9194\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.2190 - accuracy: 0.9200\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2176 - accuracy: 0.9232\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2063 - accuracy: 0.9250\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 27s 21ms/step - loss: 0.2133 - accuracy: 0.9236\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.2049 - accuracy: 0.9251\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.2060 - accuracy: 0.9259\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.1964 - accuracy: 0.9298\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.1980 - accuracy: 0.9291\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 27s 22ms/step - loss: 0.1876 - accuracy: 0.9344\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.1975 - accuracy: 0.9282\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.1984 - accuracy: 0.9294\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.1915 - accuracy: 0.9330\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 3.7390 - accuracy: 0.5617\n",
            "Test Loss: 3.7390143871307373\n",
            "Test Accuracy: 0.5616999864578247\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LeNet architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile the model# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Function to compute the rotation matrix\n",
        "def compute_rotation_matrix(weights):\n",
        "    _, _, vh = np.linalg.svd(weights)\n",
        "    return vh.T\n",
        "\n",
        "# Function to rotate the weights using the rotation matrix\n",
        "def rotate_weights(weights, rotation_matrix):\n",
        "    return np.dot(weights, rotation_matrix)\n",
        "\n",
        "# Function to compute important directions using PCA\n",
        "def compute_important_directions(weights, k):\n",
        "    pca = PCA(n_components=k)\n",
        "    pca.fit(weights.T)\n",
        "    return pca.components_.T\n",
        "\n",
        "# Function to construct the rotation matrix\n",
        "def construct_rotation_matrix(important_directions):\n",
        "    return important_directions @ important_directions.T\n",
        "\n",
        "# Function to compute the PCA-aligned weights\n",
        "def compute_pca_aligned_weights(weights, rotation_matrix):\n",
        "    return np.dot(weights, rotation_matrix)\n",
        "\n",
        "# Function to compute the regularization term\n",
        "def compute_regularization_term(important_directions_all_tasks, lambda_reg):\n",
        "    return lambda_reg * np.sum(important_directions_all_tasks, axis=2)\n",
        "\n",
        "# Function to train the network with regularization\n",
        "def train_network_with_regularization(x, y, model, pca_aligned_weights, regularization_term, epochs):\n",
        "    model.set_weights(pca_aligned_weights)\n",
        "    # Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Algorithm implementation\n",
        "def rotated_fisher_information_matrix(x_train, y_train, model, num_tasks, k, lambda_reg, learning_rate, epochs):\n",
        "    weights_all_tasks = []\n",
        "    important_directions_all_tasks = []\n",
        "\n",
        "    for task in range(num_tasks):\n",
        "        # Train the network on the current task\n",
        "        history=model.fit(x_train,y_train, batch_size=32, epochs=5)\n",
        "\n",
        "        # Compute the rotation matrix and rotate the weights\n",
        "        weights = np.concatenate([layer.get_weights()[0].flatten() for layer in model.layers])\n",
        "        rotation_matrix = compute_rotation_matrix(weights)\n",
        "        rotated_weights = rotate_weights(weights, rotation_matrix)\n",
        "\n",
        "        # Compute the important directions using PCA\n",
        "        important_directions = compute_important_directions(rotated_weights, k)\n",
        "        important_directions_all_tasks.append(important_directions)\n",
        "\n",
        "        # Construct the rotation matrix and align the weights\n",
        "        rotation_matrix_i = construct_rotation_matrix(important_directions)\n",
        "        aligned_weights = compute_pca_aligned_weights(rotation_matrix_i)\n",
        "# Function to compute the REWC loss\n",
        "def compute_rewc_loss(model, fisher_diagonal, prev_weights, prev_task_loss, lambda_rewc):\n",
        "    rew_losses = []\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        weights = layer.get_weights()[0]\n",
        "        rew_losses.append(lambda_rewc * np.sum(np.multiply(fisher_diagonal[i], np.square(weights - prev_weights[i]))))\n",
        "    return prev_task_loss + np.sum(rew_losses)\n",
        "\n",
        "# Train the network with REWC regularization\n",
        "def train_network_with_rewc(x, y, model, fisher_diagonal, prev_weights, prev_task_loss, lambda_rewc, epochs):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    rew_loss = compute_rewc_loss(model, fisher_diagonal, prev_weights, prev_task_loss, lambda_rewc)\n",
        "    return rew_loss\n",
        "\n",
        "# Rotated Fisher Information Matrix with Rotation and PCA Algorithm\n",
        "def rotated_fisher_information_matrix(x_train, y_train, x_test, y_test, model, num_tasks, k, lambda_reg, lambda_rewc, learning_rate, epochs):\n",
        "    weights_all_tasks = []\n",
        "    important_directions_all_tasks = []\n",
        "    fisher_diagonal_all_tasks = []\n",
        "    prev_task_loss = 0\n",
        "\n",
        " # Set hyperparameters\n",
        "num_tasks = 7\n",
        "k = 10\n",
        "lambda_rewc = 100\n",
        "lambda_reg = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "for task in range(num_tasks):\n",
        "    # Train the network on the current task\n",
        "    history = model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
        "        # Function to compute the rotation matrix\n",
        "def compute_rotation_matrix(weights):\n",
        "        _, _, vh = np.linalg.svd(weights.reshape(-1, 784))\n",
        "        return vh.T\n",
        "\n",
        "        # Compute the rotation matrix and rotate the weights\n",
        "        weights = np.concatenate([layer.get_weights()[0].flatten() for layer in model.layers])\n",
        "        rotation_matrix = compute_rotation_matrix(weights)\n",
        "        rotated_weights = rotate_weights(weights, rotation_matrix)\n",
        "\n",
        "        # Compute the important directions using PCA\n",
        "        important_directions = compute_important_directions(rotated_weights, k)\n",
        "        important_directions_all_tasks.append(important_directions)\n",
        "\n",
        "        # Construct the rotation matrix and align the weights\n",
        "        rotation_matrix_i = construct_rotation_matrix(important_directions)\n",
        "        aligned_weights = compute_pca_aligned_weights(rotated_weights, rotation_matrix_i)\n",
        "\n",
        "        # Compute the Fisher diagonal for REWC\n",
        "        fisher_diagonal = compute_fisher_diagonal(aligned_weights)\n",
        "\n",
        "        # Train the network with REWC regularization\n",
        "        rew_loss = train_network_with_rewc(x_train, y_train, model, fisher_diagonal, weights, prev_task_loss, lambda_rewc, epochs)\n",
        "        prev_task_loss = rew_loss\n",
        "\n",
        "        # Store the weights and Fisher diagonal\n",
        "        weights_all_tasks.append(aligned_weights)\n",
        "        fisher_diagonal_all_tasks.append(fisher_diagonal)\n",
        "\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training of three tasks sequentially and observe the catastrophic forgeeting issue"
      ],
      "metadata": {
        "id": "Nimu5m3ZGo-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Constants\n",
        "NUM_CLASSES = 10\n",
        "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 32, 32, 3\n",
        "LAMBDA = 0.4  # EWC regularization strength\n",
        "PCA_COMPONENTS = 100  # Number of PCA components\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "\n",
        "# Flatten the images for PCA\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=PCA_COMPONENTS)\n",
        "x_train_pca = pca.fit_transform(x_train_flat)\n",
        "x_test_pca = pca.transform(x_test_flat)\n",
        "\n",
        "# Reshape back to image shape with reduced components\n",
        "x_train_pca = x_train_pca.reshape(-1, PCA_COMPONENTS)\n",
        "x_test_pca = x_test_pca.reshape(-1, PCA_COMPONENTS)\n",
        "\n",
        "# Class Task Definitions\n",
        "tasks = [(0, 1, 2), (3, 4, 5), (6, 7, 8, 9)]\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=2000):\n",
        "        self.buffer = []\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def add(self, data):\n",
        "        if len(self.buffer) >= self.max_size:\n",
        "            self.buffer = self.buffer[len(data):] + data\n",
        "        else:\n",
        "            self.buffer.extend(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) == 0:\n",
        "            return np.array([]), np.array([])\n",
        "        indices = np.random.choice(len(self.buffer), batch_size)\n",
        "        x, y = zip(*[self.buffer[i] for i in indices])\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "def create_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=(PCA_COMPONENTS,)))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Initialize the replay buffer\n",
        "replay_buffer = ReplayBuffer(max_size=2000)\n",
        "\n",
        "# Placeholder for EWC variables\n",
        "prev_task_vars = []\n",
        "prev_task_fisher = []\n",
        "\n",
        "# Function to compute Fisher Information Matrix\n",
        "def compute_fisher(model, x, y):\n",
        "    fisher = []\n",
        "    for layer in model.trainable_weights:\n",
        "        fisher.append(np.zeros(layer.shape))\n",
        "\n",
        "    # Use GradientTape for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=False)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "    for i, grad in enumerate(grads):\n",
        "        fisher[i] += grad.numpy() ** 2\n",
        "\n",
        "    return fisher\n",
        "\n",
        "# Function to compute EWC loss\n",
        "def ewc_loss(model, prev_task_vars, prev_task_fisher):\n",
        "    ewc_loss_value = 0\n",
        "    for var, fisher, prev_var in zip(model.trainable_weights, prev_task_fisher, prev_task_vars):\n",
        "        ewc_loss_value += tf.reduce_sum(fisher * (var - prev_var) ** 2)\n",
        "    return ewc_loss_value\n",
        "\n",
        "# Training loop for each task\n",
        "for task_index, classes in enumerate(tasks):\n",
        "    print(f\"Training Task {task_index + 1} - Classes: {classes}\")\n",
        "    model = create_model()\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Filter data for the current task\n",
        "    task_indices = np.isin(y_train, classes)\n",
        "    x_task, y_task = x_train_pca[task_indices], y_train[task_indices]\n",
        "\n",
        "    # Update labels to be zero-indexed\n",
        "    y_task = np.array([np.where(classes == y)[0][0] for y in y_task])\n",
        "\n",
        "    # Training the model on the current task\n",
        "    for epoch in range(10):\n",
        "        model.fit(x_task, y_task, epochs=1, validation_split=0.1, verbose=1)\n",
        "\n",
        "        # Add current task data to replay buffer\n",
        "        replay_buffer.add(list(zip(x_task, y_task)))\n",
        "\n",
        "        # Sample from replay buffer and retrain\n",
        "        x_replay, y_replay = replay_buffer.sample(len(x_task) // 10)\n",
        "        if len(x_replay) > 0:\n",
        "            model.fit(x_replay, y_replay, epochs=1, verbose=1)\n",
        "\n",
        "    # Store variables and Fisher Information Matrix for EWC\n",
        "    if task_index > 0:\n",
        "        prev_task_vars.append([var.numpy() for var in model.trainable_weights])\n",
        "        prev_task_fisher.append(compute_fisher(model, x_task, y_task))\n",
        "\n",
        "    # Evaluate the model on the current task\n",
        "    test_indices = np.isin(y_test, classes)\n",
        "    x_test_task, y_test_task = x_test_pca[test_indices], y_test[test_indices]\n",
        "    y_test_task = np.array([np.where(classes == y)[0][0] for y in y_test_task])\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(x_test_task, y_test_task, verbose=1)\n",
        "    print(f\"Task {task_index + 1} - Test Results\")\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Re-evaluation on previous tasks\n",
        "    for prev_task_index in range(task_index):\n",
        "        prev_classes = tasks[prev_task_index]\n",
        "        prev_test_indices = np.isin(y_test, prev_classes)\n",
        "        x_prev_test, y_prev_test = x_test_pca[prev_test_indices], y_test[prev_test_indices]\n",
        "        y_prev_test = np.array([np.where(prev_classes == y)[0][0] for y in y_prev_test])\n",
        "\n",
        "        prev_test_loss, prev_test_accuracy = model.evaluate(x_prev_test, y_prev_test, verbose=1)\n",
        "        print(f\"Task {task_index + 1} - Re-evaluation on Task {prev_task_index + 1}\")\n",
        "        print(f\"Test Loss: {prev_test_loss}\")\n",
        "        print(f\"Test Accuracy: {prev_test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBu9gKq2h3hH",
        "outputId": "dd10273e-f294-4ed3-f1c8-42e28cdead82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n",
            "Training Task 1 - Classes: (0, 1, 2)\n",
            "422/422 [==============================] - 3s 4ms/step - loss: 0.6662 - accuracy: 0.7302 - val_loss: 0.5539 - val_accuracy: 0.7887\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.8073\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.4795 - accuracy: 0.8139 - val_loss: 0.4926 - val_accuracy: 0.7960\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8573\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3844 - accuracy: 0.8549 - val_loss: 0.4358 - val_accuracy: 0.8320\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8907\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.3098 - accuracy: 0.8834 - val_loss: 0.4455 - val_accuracy: 0.8387\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9133\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.2467 - accuracy: 0.9071 - val_loss: 0.4397 - val_accuracy: 0.8393\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9447\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.1886 - accuracy: 0.9293 - val_loss: 0.4919 - val_accuracy: 0.8353\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9453\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.1439 - accuracy: 0.9485 - val_loss: 0.5003 - val_accuracy: 0.8420\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1406 - accuracy: 0.9540\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.1113 - accuracy: 0.9605 - val_loss: 0.4814 - val_accuracy: 0.8447\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9600\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0880 - accuracy: 0.9703 - val_loss: 0.5062 - val_accuracy: 0.8613\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9653\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0711 - accuracy: 0.9766 - val_loss: 0.4990 - val_accuracy: 0.8700\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9640\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 0.7018 - accuracy: 0.8293\n",
            "Task 1 - Test Results\n",
            "Test Loss: 0.7017877697944641\n",
            "Test Accuracy: 0.8293333053588867\n",
            "Training Task 2 - Classes: (3, 4, 5)\n",
            "422/422 [==============================] - 2s 3ms/step - loss: 0.9368 - accuracy: 0.5493 - val_loss: 0.8628 - val_accuracy: 0.5813\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.8070 - accuracy: 0.6140\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.7860 - accuracy: 0.6339 - val_loss: 0.7970 - val_accuracy: 0.6133\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.6991 - accuracy: 0.6667\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.6991 - accuracy: 0.6887 - val_loss: 0.8104 - val_accuracy: 0.6453\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.6206 - accuracy: 0.7287\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.6195 - accuracy: 0.7328 - val_loss: 0.7962 - val_accuracy: 0.6333\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.5564 - accuracy: 0.7727\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.5346 - accuracy: 0.7781 - val_loss: 0.8516 - val_accuracy: 0.6340\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.8280\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.4537 - accuracy: 0.8146 - val_loss: 0.8874 - val_accuracy: 0.6353\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8480\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.3683 - accuracy: 0.8573 - val_loss: 0.9278 - val_accuracy: 0.6353\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.3048 - accuracy: 0.8940\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2936 - accuracy: 0.8929 - val_loss: 0.9444 - val_accuracy: 0.6713\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.2489 - accuracy: 0.9207\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.2245 - accuracy: 0.9185 - val_loss: 1.0080 - val_accuracy: 0.6587\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.9220\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.1740 - accuracy: 0.9435 - val_loss: 1.0658 - val_accuracy: 0.6720\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.1902 - accuracy: 0.9407\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 1.3413 - accuracy: 0.6063\n",
            "Task 2 - Test Results\n",
            "Test Loss: 1.3413347005844116\n",
            "Test Accuracy: 0.606333315372467\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 3.8427 - accuracy: 0.2880\n",
            "Task 2 - Re-evaluation on Task 1\n",
            "Test Loss: 3.842710494995117\n",
            "Test Accuracy: 0.2879999876022339\n",
            "Training Task 3 - Classes: (6, 7, 8, 9)\n",
            "563/563 [==============================] - 3s 4ms/step - loss: 0.7221 - accuracy: 0.7228 - val_loss: 0.6418 - val_accuracy: 0.7655\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.8095\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.5100 - accuracy: 0.8129 - val_loss: 0.5751 - val_accuracy: 0.7860\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.4190 - accuracy: 0.8460\n",
            "563/563 [==============================] - 3s 4ms/step - loss: 0.4065 - accuracy: 0.8510 - val_loss: 0.5319 - val_accuracy: 0.7935\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3141 - accuracy: 0.8865\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.3265 - accuracy: 0.8817 - val_loss: 0.5083 - val_accuracy: 0.8265\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.9075\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.2530 - accuracy: 0.9106 - val_loss: 0.5001 - val_accuracy: 0.8270\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9385\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.1942 - accuracy: 0.9314 - val_loss: 0.5121 - val_accuracy: 0.8340\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9495\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.1518 - accuracy: 0.9456 - val_loss: 0.5611 - val_accuracy: 0.8265\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9625\n",
            "563/563 [==============================] - 2s 4ms/step - loss: 0.1085 - accuracy: 0.9635 - val_loss: 0.5610 - val_accuracy: 0.8375\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9720\n",
            "563/563 [==============================] - 2s 4ms/step - loss: 0.0815 - accuracy: 0.9723 - val_loss: 0.5952 - val_accuracy: 0.8430\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.9645\n",
            "563/563 [==============================] - 2s 3ms/step - loss: 0.0611 - accuracy: 0.9817 - val_loss: 0.6109 - val_accuracy: 0.8300\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1379 - accuracy: 0.9585\n",
            "125/125 [==============================] - 0s 2ms/step - loss: 0.7572 - accuracy: 0.8108\n",
            "Task 3 - Test Results\n",
            "Test Loss: 0.757237434387207\n",
            "Test Accuracy: 0.8107500076293945\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 10.6241 - accuracy: 0.1067\n",
            "Task 3 - Re-evaluation on Task 1\n",
            "Test Loss: 10.624130249023438\n",
            "Test Accuracy: 0.1066666692495346\n",
            "94/94 [==============================] - 0s 2ms/step - loss: 5.8651 - accuracy: 0.2900\n",
            "Task 3 - Re-evaluation on Task 2\n",
            "Test Loss: 5.865112781524658\n",
            "Test Accuracy: 0.28999999165534973\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}